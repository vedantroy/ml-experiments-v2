{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('tiny_nerf_data.npz'):\n",
    "    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "# Each image is a different view of the same scene\n",
    "imgs = data['images']\n",
    "# Each pose is a 4x4 transformation matrix\n",
    "poses = data['poses']\n",
    "# Focal length of the camera\n",
    "# Seems to mean the distance from the camera to the image plane\n",
    "# TODO: Given this is a constant, why do we care?\n",
    "focal = data['focal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3) (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
    "    rays_d = tf.reduce_sum(dirs[..., tf.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "def get_rays_torch(H, W, focal, c2w):\n",
    "    # https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy\n",
    "    # Summary: we want to represent a grid of points in 2D space\n",
    "    # i[0, 0] is the x coordinate of the top-left point in the grid\n",
    "    # (top b/c y-axis points down)\n",
    "    # j[0, 0] is the y coordinate of the top-left point in the grid\n",
    "    # i[0, 1] is the x coordinate of the 2nd point in the top row of the grid\n",
    "    # j[0, 1] is the y coordinate of the 2nd point in the top row of the grid\n",
    "    # Another explanation: torch.arange(W) is drawing the x-coordinate (vertical) lines of the grid\n",
    "    # torch.arange(H) is drawing the y-coordinate (horizontal) lines of the grid\n",
    "    # the pair (i[0, 0], j[0, 0]) is xy coordinates of the intersection of the first vertical (x) and first horizontal (y) line\n",
    "    # the pair (i[0, 1], j[0, 0]) is non-sensical\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32), indexing='xy')\n",
    "\n",
    "    assert i.shape == (H, W) and j.shape == (H, W)\n",
    "    assert (i[0] == torch.arange(W, dtype=torch.float32)).all()\n",
    "    assert (i[1] == torch.arange(W, dtype=torch.float32)).all()\n",
    "    assert (j[0] == torch.zeros(H, dtype=torch.float32)).all()\n",
    "    assert (j[1] == torch.ones(H, dtype=torch.float32)).all()\n",
    "\n",
    "    # The first element computes the x-direction of the rays by taking the difference between the x-coordinate \n",
    "    # of each point and the center of the image (which is at W * 0.5), and then dividing by the focal length. \n",
    "    # The second element computes the y-direction of the rays using a similar formula, but with a negative sign \n",
    "    # to account for the fact that the y-axis is pointing downwards in the image. Finally, the third element is \n",
    "    # a vector of negative ones, representing the z-direction of the rays, since we assume that the camera is looking \n",
    "    # towards the negative z-axis.\n",
    "    ray_dirs = torch.stack([(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)], dim=-1)\n",
    "\n",
    "    # Each direction is a 3D vector composed of (x, y, z) coordinates\n",
    "    assert ray_dirs.shape == (H, W, 3)\n",
    "\n",
    "    # c2w (camera-to-world) is a 4x4 rotation matrix\n",
    "    # [ R  t ]\n",
    "    # [ 0  1 ]\n",
    "    R, t = c2w[:3, :3], c2w[:3, -1]\n",
    "\n",
    "    origin_points_of_rays = repeat(t, 'c -> h w c', h=H, w=W)\n",
    "    direction_of_rays = ray_dirs @ R.T\n",
    "    assert origin_points_of_rays.shape == (H, W, 3) and direction_of_rays.shape == (H, W, 3)\n",
    "    return origin_points_of_rays, direction_of_rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the two functions are equivalent\n",
    "H, W = imgs[0].shape[:2]\n",
    "tf_rays_o, tf_rays_d = get_rays(H, W, focal, poses[0])\n",
    "torch_rays_o, torch_rays_d = get_rays_torch(H, W, torch.from_numpy(focal), torch.from_numpy(poses[0]))\n",
    "\n",
    "# convert all numpy arrays to torch tensors\n",
    "tf_rays_o = torch.from_numpy(tf_rays_o.numpy())\n",
    "tf_rays_d = torch.from_numpy(tf_rays_d.numpy())\n",
    "\n",
    "# use torch testing\n",
    "torch.testing.assert_close(tf_rays_o, torch_rays_o)\n",
    "torch.testing.assert_close(tf_rays_d, torch_rays_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:57:19) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40dd8def129a866cf26ffe944144c917163dac185fb1002c02b46b964ff431fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
